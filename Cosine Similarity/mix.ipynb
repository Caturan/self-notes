{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a3daf61",
   "metadata": {},
   "source": [
    "# Cosine Similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312dd08b",
   "metadata": {},
   "source": [
    "## Code Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f9e0a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = \"The weather looks quite sunny today.\"\n",
    "sentence_2 = \"The weather is always sunny these days.\" \n",
    "sentence_3 = \"The weather wasn't very sunny yesterday.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095ec51",
   "metadata": {},
   "source": [
    "- First, we need to create a vector for each sentence and create word list in the three sentences. \n",
    "- To do this, creating a simple function that will tokenize the sentences by stripping them of any punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "487986a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sunny', 'these', 'looks', 'yesterday', 'wasnt', 'weather', 'is', 'always', 'quite', 'the', 'days', 'very', 'today']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def tokenizer(text):\n",
    "    # Remove punctuation\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    # Convert to lowercase to treat words like 'The' and 'the' as the same\n",
    "    text_lower = text_nopunct.lower()\n",
    "\n",
    "    # Split the text on one or more whitespace characters\n",
    "    tokens = re.split(r'\\s+', text_lower.strip())\n",
    "    \n",
    "    # Filter out any empty strings that might result from splitting\n",
    "    tokens = [token for token in tokens if token]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "word_list = list(set(tokenizer(sentence_1) + \n",
    "                     tokenizer(sentence_2) + \n",
    "                     tokenizer(sentence_3)))\n",
    "\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33b3db1",
   "metadata": {},
   "source": [
    "- In the next step, create a function that will assign a value of 1 to the vector of each sentence if it contains the words in the word list, and 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "661622a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_1: [1 0 1 0 0 1 0 0 1 1 0 0 1]\n",
      "vector_2: [1 1 0 0 0 1 1 1 0 1 1 0 0]\n",
      "vector_3: [1 0 0 1 1 1 0 0 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "vector_1 = [0]*len(word_list)\n",
    "vector_2 = [0]*len(word_list)\n",
    "vector_3 = [0]*len(word_list)\n",
    "\n",
    "def vectors(word_list, sentence, vector):\n",
    "    for j in tokenizer(sentence):\n",
    "        for i in range(0, len(word_list)):\n",
    "            if j == word_list[i]:\n",
    "                vector[i] = 1\n",
    "    vector = np.array(vector)\n",
    "    return vector\n",
    "\n",
    "vector_1 = vectors(word_list, sentence_1, vector_1)\n",
    "vector_2 = vectors(word_list, sentence_2, vector_2)\n",
    "vector_3 = vectors(word_list, sentence_3, vector_3)\n",
    "\n",
    "print(\"vector_1:\", vector_1)\n",
    "print(\"vector_2:\", vector_2)\n",
    "print(\"vector_3:\", vector_3)"
   ]
  },
  {
   "attachments": {
    "1_hhgmobNsZ8AoN_XSn4ptPw.webp": {
     "image/webp": "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"
    }
   },
   "cell_type": "markdown",
   "id": "cfe5f56c",
   "metadata": {},
   "source": [
    "- ![1_hhgmobNsZ8AoN_XSn4ptPw.webp](attachment:1_hhgmobNsZ8AoN_XSn4ptPw.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6de29d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between sentence_1 and sentence_2:  0.4629100498862757\n",
      "Cosine similarity between sentence_1 and sentence_3 : 0.5000000000000001\n",
      "Cosine similarity between sentence_2 and sentence_3 : 0.4629100498862757\n"
     ]
    }
   ],
   "source": [
    "def cos_sim(vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_1 = np.linalg.norm(vector1)\n",
    "    norm_2 = np.linalg.norm(vector2)\n",
    "    return dot_product / (norm_1 * norm_2)\n",
    "\n",
    "print(\"Cosine similarity between sentence_1 and sentence_2: \", cos_sim(vector_1, vector_2))\n",
    "print(\"Cosine similarity between sentence_1 and sentence_3 :\", cos_sim(vector_1, vector_3))\n",
    "print(\"Cosine similarity between sentence_2 and sentence_3 :\", cos_sim(vector_2, vector_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e244e",
   "metadata": {},
   "source": [
    "- Let's make with Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3083d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"The weather looks quite sunny today.\",\n",
    "    \"The weather is always sunny these days.\",\n",
    "    \"The weather wasn't very sunny yesterday.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bf03bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>always</th>\n",
       "      <th>days</th>\n",
       "      <th>is</th>\n",
       "      <th>looks</th>\n",
       "      <th>quite</th>\n",
       "      <th>sunny</th>\n",
       "      <th>the</th>\n",
       "      <th>these</th>\n",
       "      <th>today</th>\n",
       "      <th>very</th>\n",
       "      <th>wasn</th>\n",
       "      <th>weather</th>\n",
       "      <th>yesterday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vector_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vector_2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vector_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          always  days  is  looks  quite  sunny  the  these  today  very  \\\n",
       "vector_1       0     0   0      1      1      1    1      0      1     0   \n",
       "vector_2       1     1   1      0      0      1    1      1      0     0   \n",
       "vector_3       0     0   0      0      0      1    1      0      0     1   \n",
       "\n",
       "          wasn  weather  yesterday  \n",
       "vector_1     0        1          0  \n",
       "vector_2     0        1          0  \n",
       "vector_3     1        1          1  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import pandas as pd \n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix,\n",
    "                  columns=count_vectorizer.get_feature_names_out(),\n",
    "                  index=['vector_1', 'vector_2', 'vector_3'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6661977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46291005, 0.5       ],\n",
       "       [0.46291005, 0.        , 0.46291005],\n",
       "       [0.5       , 0.46291005, 0.        ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine = cosine_similarity(df, df)\n",
    "np.fill_diagonal(cosine, 0)\n",
    "cosine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
