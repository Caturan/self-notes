{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Consideration for Fine-Tuning \n",
    "1. Data Quality and Quantity \n",
    "    - High-Quality Data: Fine tuning requires a substantial amount of clean, annotated audio data relevant to the target domain or language. \n",
    "    - Data Diversity: Including diverse speakers, accents, and audio conditions in the fine-tuning dataset enhances the model's generalization capabilities. \n",
    "\n",
    "2. Computational Resource: \n",
    "    - Hardware Requirements: Larger models like Whisper Large v3 demand significant computational power, including GPUs with ample memory, for both training and inference. \n",
    "    - Training Time: Fine tuning can be time-consuming, so planning for sufficient training time is essential.\n",
    "\n",
    "3. Overfitting and Generalization \n",
    "    - Regularization Techniques: Employ methods such as dropout or data augmentation to prevent overfitting to the fine-tuning dataset. \n",
    "    - Validation Sets: Use separate validation data to monitor the model's performance and ensure it generlizes well to unseen data. \n",
    "\n",
    "4. Evaluation Metrics: \n",
    "    - Word Error Rate (WER): Commonly used to assess the accuracy of ASR systems by comparing the transcribed text to reference transcripts. \n",
    "    - Character Error Rate (CER): Useful for languages where word boundaries are not well-define or for fine-grained error analysis. \n",
    "\n",
    "5. Model Size Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The image illustrates the architecture of OpenAI's Whisper, a versatile transformer model designed for automatic speech recognition (ASR). \n",
    "- Whisper's architecture integrates an encode-decoder structure, similar to many tranformet-based models, and is optimized for transcribing audio into text. \n",
    "- Below is an explanation of its key components and workflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Input Processing \n",
    "    - The input to Whisper is Log-mel spectrograms, a common representation of audio used in ASR tasks. The audio waveform is first converted into spectrograms, representing the audio's frequency components over time.\n",
    "    - To enhance model performance, the spectrograms pass through a series of 2D Convolutional layers with Gated Linear Units (GELU) activations, capturing local temporal patterns in the audio.\n",
    "    - The output is then combined with sinusoidal positional encodings, which provide information about the position of the features in the input sequence, helping the model maintain temporal awareness of the audio data.\n",
    "\n",
    "2. Encoder Blocks: \n",
    "    - Whisper's encoder consist of multiple stacked transformer blocks that progressively process the input features. \n",
    "    - Each block contains self-attention mechanisms to capture complex dependencies within the input sequence and feed-forward neural networks to further refine the features. \n",
    "    - The encoder's output, which encodes the audio features into latent representations, serves as input to the decoder through a cross-attention mechanism.\n",
    "\n",
    "3. Decoder Blocks: \n",
    "    - The decoder receives the encoder's output and additional input in the form of tokens from the multitask training format. \n",
    "    - The decoder bloks utilize learned positional encodings that allow the model to predict text sequentially, handling text generation in a manner similar to other language models like GPT.\n",
    "    - The decoder employs cross-attention to interact with the endoder's output, enabling the model to leverage audiÄ± representations for predicting the next token in the transcription. \n",
    "\n",
    "4. Multitask Training: \n",
    "    - Whisper is trained with a multitask objective, where the input includes tokens for tasks such as transcription, translation, or language identification. This allows the model to perform various language processing tasks using the same architecture. \n",
    "    - The multitask format starts with \"SOT\" (Start-of-Transcript), followed by the language token (eg., \"EN\" for English), the task token (\"TRANSCRIBE\"), ant the output sequence of words. \n",
    "\n",
    "5. Next Token Prediction: \n",
    "    - The decoder generates tokens iteratively, predicting one token at a time based on the context provided by both the input audio features and the previously generated tokens. \n",
    "    - As shown in the image, the next-token prediction mechanism is aligned to produce words step-by-step, ensuring accurate transcription. \n",
    "\n",
    "6. Fine-Tuning Whisper: \n",
    "    - Fine-tuning involves training the model further on domain-specific or language-specific data, refining the weights of both the encoder and decoder componenets. \n",
    "    - During fine-tuning, all architectural components, including the convolutional layers, attention layers, and positional embeddings, are optimized to enhance the model's ability to transcribe specialized speech accurately. \n",
    "\n",
    "- This architecture, with its encoder-decoder design and multitask training capabilities, is what makes Whisper highly adaptable for various ASR tasks, from general transcription to domain-specific speech recognition. Fine tuning Whisper further customizes this structure to meet specific use cases, improving performance on language variants, domain-specific terminology, or noisy audio conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding\n",
    "- Let's us get started with the coding part. Below I am listing my pointer for the training. \n",
    "    1. I had to do multiple runs to get it in a desirable resulting state, so don't expect it work in one go. The model's performance stritcly depends on training and quality of data that you give for training.\n",
    "    2. I had used A100 GPU, which had 40 GB of VRAM for model training and a high configuration CPU coming with the A100 GPU. \n",
    "    3. I had 50+ hours of data in .wav format for model training, that doesn't means I used everything at one go. I did it in a selective manner to fine tune the key places where the base model was weak. \n",
    "    4. The OpenAI Whisper Large V3 has a context length of 448 tokens, which is like 25 to 40 seconds of audio length. So training the model on audios bigger than this means, it will truncate anything after 30 seconds. \n",
    "    5. Profile your audios to be 30 seconds, 16kHz frequency and laslty the audio should be in .wav format for better performance. \n",
    "    6. I have a Google Collab Pro+ for my model training. If your data is less, you can go for free version, the version gives you 3 hours of run time with replenishments arriving in every 24 hours. \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
