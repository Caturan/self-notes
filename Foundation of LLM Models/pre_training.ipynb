{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cefba8d8",
   "metadata": {},
   "source": [
    "# Foundation of LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df29bd",
   "metadata": {},
   "source": [
    "## Pre-face \n",
    "- a variable\n",
    "- a row vector or matrix\n",
    "- f(a) function of a\n",
    "- max f(a) maximum value of f(a)\n",
    "- argmaxa f(a) value of a that maximizes f(a)\n",
    "- x input token sequence to a model\n",
    "- xj input token at position j\n",
    "- y output token sequence produced by a model\n",
    "- yi output token at position i\n",
    "- θ model parameters\n",
    "- Pr(a) probability of a\n",
    "- Pr(a|b) conditional probability of a given b\n",
    "- Pr(·|b) probability distribution of a variable given b\n",
    "- Prθ(a) probability of a as parameterized by θ\n",
    "- ht hidden state at time step t in sequential models\n",
    "- H matrix of all hidden states over time in a sequence\n",
    "- Q, K, V query, key, and value matrices in attention mechanisms\n",
    "- Softmax(A) Softmax function that normalizes the input vector or matrix A\n",
    "- L loss function\n",
    "- D dataset used for training or fine-tuning a model\n",
    "- ∂L/∂θ gradient of the loss function L with respect to the parameters θ\n",
    "- KL(p || q) KL divergence between distributions p and q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb707f1",
   "metadata": {},
   "source": [
    "## Pre-training\n",
    "- The development of neural sequence models, such as Transformer, along with the improvements in large-scale self-supervised learning, has opened the door to universal language understanding and generation. \n",
    "- Large-scale research on pre-training in NLP began with the development of language models using self-supervised learning. \n",
    "- This family of models covers several well-known examples like BERT and GPT, all with a similar idea that general language understanding and generation can be achieved by training models to predict masked words in a huge amount of text. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
