{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ad9dc1",
   "metadata": {},
   "source": [
    "# What is zero-shot learning? \n",
    "- Zero-shot learning (ZSL) is a machine learning scenario in which an AI model is trained to recognize and categorize objects or concepts without having seen any examples of those categories or concepts beforehand. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ae3f7",
   "metadata": {},
   "source": [
    "- Most state-of-the-art deep learning models for classification or regression are trained through supervised learning, which requires many labeled examples of relevant data classes. \n",
    "- Models \"learn\" by making predictions on a labeled training dataset; data labels provide both the range of possible anwers and the correct answers (or ground truth) for each training example. \n",
    "- \"Learning\" here, means adjusting model weights to minimize the difference between the model's predictions and that ground truth.\n",
    "- This process requires enough labeled samples for many rounds of training and updates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7527462",
   "metadata": {},
   "source": [
    "- While powerful, supervised learning is impractical in some real-world scenarios. \n",
    "- Annotating large amounts of data samples is costly and time consuming, and in cases like rare diseases and newly discovered species, examples may be scarce or non-existent. \n",
    "- Consider image recognition tasks; according to one study, humans can recognize approximately 30,000 individually distinguishable  object categories. \n",
    "- It's not feasible, in terms of time, cost and computational resources, for artificial intelligence models to remotely approach human capabilities if they must be explicitly trained on labeled data for each class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3210dce",
   "metadata": {},
   "source": [
    "- The need for machine learning models to be able to generalize quickly to a large number of semantic categories with minimal training overhead has given rise to n-shot learning: a subset of machine learning that also includes few-shot learning (FSL) and one-shot learning. \n",
    "- Few-shot learning typically uses transfer learning and meta learning-based methods to train models to quickly recognize new classes with only a few labeled training examples-or, in one-shot learning, a single labeled example.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d570b",
   "metadata": {},
   "source": [
    "- Zero-shot learning, like all n-shot learning, refers not to any specific algorithm or neural network architecture, but to the nature of the learning problem itself; in ZSL, the model is not trained on any labeled examples of the unseen classed it is asked to make on post-training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe937e3",
   "metadata": {},
   "source": [
    "## Generalized zero-shot learning (GSZL)\n",
    "- In a conventional ZSL setting, the model is tested on a dataset containing samples from unseen classes of data. \n",
    "- While useful for developing and validating zero-shot methodologies, it doesn't reflect most common real-worlds conditions: generalized zero-shot learning (GZSL) refers to the specific zero-shot-learning problem in which the data point(s) the model is tasked with classifying might belong to either unseen classes or seen classes: classes the model has already \"learned\" from labeled examples. \n",
    "- GSZL must overcome an additional challenge: tendency for classifiers to bias predictions towards classes it has seen in training over unseen classes it has not yet been exposed to. As such, GSZL often requires additional texhniques to mitigate that bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c624672",
   "metadata": {},
   "source": [
    "## How does zero-shot learning work? \n",
    "- In the absence of any labeled examples of the categories the model is being trained to learn, zero-shot learning problems make use of auxilary information: textual descriptions, attributes, embedded representations or other semantic information relevant to the task at hand. \n",
    "- Rather than directly modeling the decision boundaries between classes, zero-shot learning techniques typically output a probaility vector representing the likelihood that a given input belongs to certain classes.\n",
    "- GZSL methods may add a premilinary discriminator that first determines whether the sample belongs to a seen class or a new class, then proceed accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ebcca",
   "metadata": {},
   "source": [
    "## Understanding labels \n",
    "- In supervised learning--as well as in few-shot learning (FSL)-- the model learns to recognize different classes by directly observing one or more labeled examples of each class. \n",
    "- Without these explicit annotations to guide them, zero-shot learning requires a more fundemental understanding of the label's meaning. \n",
    "\n",
    "- For a simple analogy, imagine a child wants to learn what a bird looks like. In a process resembling supervised leaning of FSL, the child learns by looking at images labeled \"bird\" in a book of animal pictures. \n",
    "- Moving forward, she'll recognize a bird because it resembles the bird images she has already seen. \n",
    "- But in ZSL scenario, no such labeled examples are available. Instead, the child might readn an encylopedia entry on birds and learn that they are small-or medium-sized animals with feathers, beaks and wings that can fly through the air. \n",
    "- She will then be able to recognize a bird in the real world, even though she has never seen one before, because she has learned the concept of a bird. \n",
    "\n",
    "- As mentioned earlier, LLMs have demonstrated natural potential for ZSL, derived from their ability to fundementally understand the meaning of the words used to name data classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74d47bb",
   "metadata": {},
   "source": [
    "## Transfer Learning \n",
    "- To minimize the time and resources needed for training, as well the amount of auxilary information needed to identify unseen classes, ZSL often leverages transfer learning instead of training models from scratch. \n",
    "- Transfer learning is used prominently in ZSL methods represent classes and samples as semantic embeddings. \n",
    "- For example, a model performing zero-shot text classification might use a transformer-based model like BERT, already pre-trained on a massive corpus of language data, to convert words into vector embeddings. \n",
    "- Likewise, a zero-shot image classification model might repurpose a pre-trained convolutional neural network (CNN) like a ResNet or U-Net, as it will already have learned filter weights conducive to identifying important image features that could inform classification. \n",
    "- For example, imagine and object detection model has already learned to recognize grizzly bears. Instead of training it to also recognize polar bears by providing it with labeled examples of polar bears, it can be trained to understand that polar bears look like grizzly bears with white fur. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03102be6",
   "metadata": {},
   "source": [
    "## Attribute-based methods \n",
    "- Attribute-based zero-shot learning methods use logic similar to that of conventional supervised learning. Rather than directly training a classifier on labeled examples of each data class, classifiers are trained on labeled features of certain data classes, like color, shape or other key characteristics. \n",
    "- Though the target classes are not directly seen in training, the label of an unseen class can be inferred if its attributes resemble attribute classes present in the training data. \n",
    "- Once the classifier has learned all relevant features, it can utilize semantic descriptions of different classes. This approach is particularly useful when labeled examples of a target class are unavailable, but labeled examples of its chareacteristic features are relatively abundant. \n",
    "- For example, a model can learn \"stripes\" from images of tigers and zebras; it can learn \"yellow\" from images of canaries, and \"flying insect\" from images of flies. The model can now perform zero-shot classification of bees, despite the absence of bee images in the training set, because it can understand them as a combination of learned features: \"yellow, striped flying insects.\" \n",
    "\n",
    "- While versatile and useful in the right circumstances, attribute-based ZSL methods have important drawbacks: \n",
    "    - They rely on the key assumption that every class can be described with a single vector of attributes, which is not always the case. \n",
    "    - Annotating examples of individual attributes can potentially be as costly and time-consuming as annotating examples of a given class. \n",
    "    - Attribute-based methods cannot generalize to classes whose attributes are unknown or not present in available samppes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d2e8c5",
   "metadata": {},
   "source": [
    "## Embedding-based methods \n",
    "- Many ZSL methods represent both classes and samples as semantic embeddings: vector representations that can be used to reflect the features or meaning of (and relationship between) different data points. \n",
    "- Classification is then determined by measuring similarity between the semantic embedding of a given sample and the embeddings of the different classes it might be categorized into. \n",
    "- Once data points have been represented as embeddings, classification is determined using principles similar to those of K-nearest neighbors algorithms: some metric of distance, like cosine similarity, Euclidian distance or Wassertein distance, is used to measure the proximity of the embedding of the input data to the embeddings for each potential class. \n",
    "- The closer (or more similar) the embeddings of that data sample is to the embedding for a given class, the more likely it belongs to that class. \n",
    "\n",
    "- These embeddings can be generated in a number of ways. For example: \n",
    "    - Pre-trained models and algorithms like BERT, word2vec or GloVe (Global Vectors) can readily output vector embeddings for words (like the names of class labels). \n",
    "    - Likewise, the encoder networks of pre-trained CNNs like ResNet (or transformers-based image encoders like ViT) can do the same for images. \n",
    "    - Autoencoders can learn latent representations-compressed, lower-dimensional encodings that isolate the most distinguishing variables of a given data input-of samples or classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb6a89",
   "metadata": {},
   "source": [
    "### Joint embedding space \n",
    "- Because embedding-based methods typically process auxilary information and vector space embeddings of different forms (or modalities) of data--like word embeddings that describe a class label and the image embedding of a photograph that might belong to that class--they require a way to facilitate comparision between embeddings of different data types. \n",
    "- To be compared, vector embeddings of different types and sizes must be normalized and projected to a shared high-dimensional semantic space, referred to as the joint embedding space, where they can be compared in an apples-to-apples setting. \n",
    "- Abstractly speaking, this works similarly to the concept of finding the least common denominator to compare unlike fractions. \n",
    "- A strong, correlative mapping between different embedding spurces is essential to a model's generalization performance. \n",
    "- Some zero-shot learning models also use contrastive learning to better align semantic embeddings from different models or algorithms: using pairs of semantic embeddings, contrastive learning trains models to minimize the distance between \"positive\" pairs (like the embedding of an image of a dog and that of the word \"dog\") and maximize the distance between \"negative\" (non-matching) pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8e73c",
   "metadata": {},
   "source": [
    "### Joint end-to-end training \n",
    "- One effective way to ensure alignment between embeddings from different models is to jointly train those models side by side. \n",
    "- For example, OpenAI's Contrastive Language-Image-Pre-training (CLIP) model was trained on an enourmous unlabeled dataset of over 400M image-caption pairs taken from the internet. \n",
    "- These pairings were used to jointly train an image encoder and text encoder from scratch, using contrastive loss to maximize the cosine similarity between embeddings and the embeddings for their corresponding captions. \n",
    "- This yielded a natural ability for zero-shot classification: with no fine-tuning, CLIP demonstrated strong classification performance on 27 different image classification datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba9d0d",
   "metadata": {},
   "source": [
    "## Generative-based methods \n",
    "- Generative AI offers an alternate solution to the zero-shot learning problem: using auxiliary information to generate sample data. \n",
    "- Generative-based methods can leverage the semantic representations of unseen classes to generate samples that, once labeled, can be used to convert the learning problem to standard supervised learning. \n",
    "- Though unlabeled samples (or representations of closel realted seen classes) can aid in the synthesis of samples, in a zero-shot setting this process often relies primarily on semantic descriptions. \n",
    "- LLMs can reduce the labor needed to produce high quality descriptions: in the release paper for its DALL-E 3 text-to-image generation model, OpenAI noted that synthetic captions even improved model performance relative to \"ground truth\" captions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6045421d",
   "metadata": {},
   "source": [
    "### Variational autoencoders\n",
    "- Variational autoencoders (VAEs) are self-supervised generative models that learn latent representations of training data as a parameterized distribution of latent variables. \n",
    "- In other words, they learn to encode a data class not as a static semantic emeddings, but as a probality distribution in latent space. \n",
    "- Conditional VAEs (CVAEs) can constrain the properties of synthesized samples by maximing the probaility of chosen variables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df28d5fa",
   "metadata": {},
   "source": [
    "### Generative Adversarial Networks (GANS)\n",
    "- GANs consist of two neural networks, jointly trained in an adversarial zero-sum game: \n",
    "- A generator that uses semantic attributes and Gaussioan noise to synthesize samples and a discriminator that determines whether samples are real or \"fake\" (that is, synthesized by the generator). \n",
    "- Feedback from the discriminator is used to train the generator until the discriminator can no longer distinguish between real and fake samples. \n",
    "- Since the original GAN paper in 2014, a number of modifications have been developed to refine and stabilize this process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebdfeaf",
   "metadata": {},
   "source": [
    "### VAEGANs \n",
    "- Both VAEs and GANs suffer from drawbacks: \n",
    "    - VAEs are stable, but tend to generate blurry images due to the nature of how samples are reconstructed from latent space. \n",
    "    - GANs learn to generate high-quality images, but are prone to destailization because they must converge two separate and distinct training process. \n",
    "- Though a number of a number of modifications have been developed to refine and stabilize both process, combining the two model architectures has yielded promising results in a zero-shot setting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c845576b",
   "metadata": {},
   "source": [
    "## Large Language Models (LLMs)\n",
    "- LLMs can also be used to synthesize labeled samples: for example, using an autoregressive model like Llama 2 to generate samples that can be used to train a bidirectional language model like Sentence-BERT for text classification tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
