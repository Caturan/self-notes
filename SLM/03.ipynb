{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1749dccd",
   "metadata": {},
   "source": [
    "# Day 3 - Building Our First Tokenizer from Scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17ffa4",
   "metadata": {},
   "source": [
    "- In that lesson, we are going to build our first tokenizer from scratch. \n",
    "- In its simplest form, the main job of a tokenizer is to break down our sentence into tokens, the atomic units that a model can understand. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df104f6",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens\n",
    "- Our goal is to tokenize our data into indiviudal words and special chracters that we can then turn into embeddings for LLM training. \n",
    "- Now, the question is: how can we create these tokens? Let's not worry about the LLMs at this stage. \n",
    "- If i gave you this sentence, \"IdeaWeaver, a comprehensive CLI tool for AI model training and evaluation\" and asked you to break it into smaller parts, how would you do it in Python? \n",
    "- The first Python method that comes to my mind is .split(), which with no argument, splits on any run of whitespace and discards it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95efc15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IdeaWeaver,', 'a', 'compherensive', 'CLI', 'tool', 'for', 'AI', 'model', 'training', 'and', 'evaluation']\n"
     ]
    }
   ],
   "source": [
    "text = \"IdeaWeaver, a compherensive CLI tool for AI model training and evaluation\"\n",
    "parts = text.split()\n",
    "print(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7c293",
   "metadata": {},
   "source": [
    "- Or, one of the most popular Python modules that comes to mind is the re module, where we use the pattern r'(\\s)' to split on any single whitespace character and capture that character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b14887b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IdeaWeaver,', ' ', 'a', ' ', 'compherensive', ' ', 'CLI', ' ', 'tool', ' ', 'for', ' ', 'AI', ' ', 'model', ' ', 'training', ' ', 'and', ' ', 'evaluation']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"IdeaWeaver, a compherensive CLI tool for AI model training and evaluation\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f6b15c",
   "metadata": {},
   "source": [
    "- Let's extend the above code further and split on whitespace (\\s), commas (,) or periods (\\.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd708cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IdeaWeaver', ',', '', ' ', 'a', ' ', 'comprehensive', ' ', 'CLI', ' ', 'tool', ' ', 'for', ' ', 'AI', ' ', 'model', ' ', 'training', ' ', 'and', ' ', 'evaluation', '.', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"IdeaWeaver, a comprehensive CLI tool for AI model training and evaluation.\"\n",
    "result = re.split(r'([\\s,\\.])', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e68c76a",
   "metadata": {},
   "source": [
    "- We can see that both the words and punctuation marks are now seperate entries in the list just as we wanted. \n",
    "- However, there is still a small issue: the list includes whitespace characters. \n",
    "- If those are not required, we can safely remove these redundant entries using the following approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a396a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IdeaWeaver', ',', 'a', 'comprehensive', 'CLI', 'tool', 'for', 'AI', 'model', 'training', 'and', 'evaluation', '.']\n"
     ]
    }
   ],
   "source": [
    "# keep only tokens that aren't empty or all whitespace \n",
    "result = [token for token in result if token.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f085acea",
   "metadata": {},
   "source": [
    "### Should We Keep or Remove Whitespace? \n",
    "- When building a simple tokenizer, the decision to keep or remove whitespace characters depends on the specific needs of our application. \n",
    "- Removing whitespace can reduce memory usage and computational overhead. \n",
    "- However, preserving whitespace is important in cases where the structure of the text matters such as when processing Python code, where indentation and spacing are critical. \n",
    "- In this example, we choose to remove whitespace for simplicity and to keep the tokenized output concise. \n",
    "- Let's refine it further to handle addtional types of punctuation such as question marks, quotation marks, and double dashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1e99ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IdeaWaver', '--', 'a', 'comprehensive', 'CLI', 'tool', 'for', 'AI', 'model', 'training', 'and', 'evaluation', '?']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "text = \"IdeaWaver-- a comprehensive CLI tool for AI model training and evaluation?\"\n",
    "tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "tokens = [tok.strip() for tok in tokens if tok.strip()]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f791c1e",
   "metadata": {},
   "source": [
    "## Creating Token IDs\n",
    "- In the previous section, we tokenized our text. \n",
    "- Let's now create a list of all unique tokens and sort them alphabetically to determine the vocabulary size: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7620801b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(set(tokens))\n",
    "vocab_size = len(all_tokens)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3647106e",
   "metadata": {},
   "source": [
    "- After determining the vocabulary size we can create the vocabulary and print it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3ef66d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--: 0\n",
      "?: 1\n",
      "AI: 2\n",
      "CLI: 3\n",
      "IdeaWaver: 4\n",
      "a: 5\n",
      "and: 6\n",
      "comprehensive: 7\n",
      "evaluation: 8\n",
      "for: 9\n",
      "model: 10\n",
      "tool: 11\n",
      "training: 12\n"
     ]
    }
   ],
   "source": [
    "vocab = {token: idx for idx, token in enumerate(all_tokens)}\n",
    "\n",
    "for token, idx in vocab.items():\n",
    "    print(f\"{token}: {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903ef73",
   "metadata": {},
   "source": [
    "- As shown in the output above, the dictionary maps each individual token to a unique integer ID.\n",
    "- In the later, when we want to convert the numeric outputs of a language model back into readable text, we will need a way to map token IDs back to their original tokens. \n",
    "- To achieve this, we can create an inverse vocabulary that reverses the mapping from token IDs to text tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c086577",
   "metadata": {},
   "source": [
    "## Step 3: Implementing a Complete Tokenizer Class \n",
    "- Let's now implement a complete Tokenizer class in Python. \n",
    "- This class will include an encode method that splits input text into tokens and maps each token to its corresponding integer ID using the vocabulary. \n",
    "- It will also provide a decode method, which performss the reverse translating token IDs back into their original text from. \n",
    "1. Keep our vocabulary (the token -> ID map) inside the tokenizer class so both encoding and decoding can use it. \n",
    "2. Make a reverse map (ID -> token) so we can turn numbers back into words. \n",
    "3. To encode, split the text into tokens, clean them up, and look up each token's ID. \n",
    "4. To decode, look up each ID's token and join them back into a string. \n",
    "5. Finally, remove any extra spaces before punctuation so the output reads naturally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dec83c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from typing import List, Dict, Pattern \n",
    "\n",
    "class BasicTokenizer: \n",
    "    \"\"\"\n",
    "    A minimal whitespace- and punctuation- based tokenizer\n",
    "    with bidirectional mapping between tokens and integers IDs. \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            token_index: Dict[str, int],\n",
    "            split_pattern: Pattern = re.compile(r'([,.:;?_!\"\\'()]|--|\\s)'),\n",
    "            rejoin_pattern: Pattern = re.compile(r'\\s+([,.:;?_!\"\\'()])')\n",
    "):\n",
    "        # forward and reverse vocab \n",
    "        self.token_index = token_index \n",
    "        self.index_token = {idx: tok for tok, idx in token_index.items()}\n",
    "\n",
    "        # patterns for tokenization and for stitching text back together \n",
    "        self._split_pattern = split_pattern\n",
    "        self._rejoin_pattern = rejoin_pattern\n",
    "\n",
    "        # optional unknown token ID \n",
    "        self.unknown_id = token_index.get(\"\", None)\n",
    "\n",
    "    def _tokenize(self, text:str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split on punctuation, double-dash, or whitespace, \n",
    "        strip out empty pieces. \n",
    "        \"\"\"\n",
    "        raw = self._split_pattern.split(text)\n",
    "        return [piece.strip() for piece in raw if piece.strip()]\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Convert a string into a list of token IDs. \n",
    "        Unknown tokens map to if present, otherwise are skipped. \n",
    "        \"\"\"\n",
    "        tokens = self._tokenize(text)\n",
    "        ids = []\n",
    "        for tok in tokens: \n",
    "            if tok in self.token_index:\n",
    "                ids.append(self.token_index[tok])\n",
    "            elif self.unknown_id is not None: \n",
    "                ids.append(self.unknown_id)\n",
    "            # else: drop it \n",
    "        return ids \n",
    "\n",
    "    def decode(self, ids: List[int]) -> str: \n",
    "        \"\"\"\n",
    "        Convert a list of IDs back into a human-readable string,\n",
    "        rejoining tokens with spaces, then fixing space-before-punctuations.\n",
    "        \"\"\"        \n",
    "        # map IDs -> tokens, skip missing IDs \n",
    "        tokens = [self.index_token[i] for i in ids if i in self.index_token]\n",
    "        text = \" \".join(tokens)\n",
    "        # remove unwanted spaces before punctuation\n",
    "        return self._rejoin_pattern.sub(r\"\\1\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed9872e",
   "metadata": {},
   "source": [
    "- Let's intantiate a tokenizer class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de480d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 5, 3, 11, 9, 2, 10, 12, 6, 8, 1]\n"
     ]
    }
   ],
   "source": [
    "# Insantiate \n",
    "tokenizer = BasicTokenizer(vocab)\n",
    "\n",
    "# Tokenize & encode \n",
    "text = \"IdeaWeaver-- a comphrensive CLI tool for AI model training and evaluation?\"\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "# See the result \n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12905915",
   "metadata": {},
   "source": [
    "- After printing out the token IDs, we can decode the original text by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9df4fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- a CLI tool for AI model training and evaluation?\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59c25f8",
   "metadata": {},
   "source": [
    "- The output above shows that the decode method succesfully translated the token IDs back into the original text. \n",
    "- Everything looks good so far, we have built a tokenizer that can tokenize and de-tokenize text based on a sample from the training set. \n",
    "- Now, let's test it on a new text sample that wasn't part of training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baa920d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with new text: 'Hello, how are you?'\n",
      "Encoded IDs: [1]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, how are you?\"\n",
    "print(f\"\\nTesting with new text: '{text}'\")\n",
    "encoded_ids = tokenizer.encode(text)\n",
    "print(\"Encoded IDs:\", encoded_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e94722",
   "metadata": {},
   "source": [
    "### What's Happening? \n",
    "- The tokenizer only recognizes tokens that were in the original vocabulary. From Hello, how are you?: \n",
    "- Hello, how, are, you --Not in vocabulary -> skipped \n",
    "- , --Not in vocabulary -> skipped \n",
    "- ? --In vocabulary (from original text) -> encoded as ID 1\n",
    "-\n",
    "- This demonstrates a fundamental limitation of this basic tokenizer: it can only work with text containing tokens from its training vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79df7ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('model', 10)\n",
      "('tool', 11)\n",
      "('training', 12)\n",
      "('<|endoftext|>', 13)\n",
      "('<|unk|>', 14)\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(tokens)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd88b43",
   "metadata": {},
   "source": [
    "## Adding Special Context Tokens \n",
    "- In the previous section, we built a simple tokenizer and applied it to a sample text. \n",
    "- Now, we will enhance that tokenizer to handle unknown words and mark the boundaries between separete texts. \n",
    "- Specifically, we will update the BasicTokenizer to support two special tokens: <|unk|> and <|endoftext|>. \n",
    "- The <|unk|> token will be used to represent any word that is not found in the vocabulary, this helps the model handle unexpected or out-of-vocabulary inputs. \n",
    "- The <|endoftext|> token will serve as a seperator between unrelated text segments. For instance, when training GPT-style language models on muktiple independent documents or books, it's common to insert a boundary token before each new text to indicate a transtion. \n",
    "- Let's go ahead and modify the vocabulary to include these two special tokens by appending them to the list of unique words we created in the previous section. \n",
    "- Up to this point, we have discussed tokenization as a critical preprocessing step for feeding text into large language models (LLMs). Depending on the model architecture and training methodology, some researches incorporate additional special tokens such as: \n",
    "    - [BOS] (Beginning of Sequence): Marks the start of a sequence, signaling the model where the input begins. \n",
    "    - [EOS] (End of Sequence): Indicates the end of a sequence. This is especially useful when concatenating multiple unrelated texts, similar to GPT's <|endoftext|> token. For example, when combining two separate Wikipedia articles or books, [EOS] marks where one ends and the next begins. \n",
    "    - [PAD] (Padding): When training with batches of varying-length texts, the shorter sequences are padded with [PAD] tokens to match the length of the longest sequence in the batch. \n",
    "- It's important to note that GPT models do not use tokens like [BOS], [EOS], or [PAD]. Instead, they rely solely on a single special token: <|endoftext|>. \n",
    "- Additionally, GPT models do not include an <|unk|> token for handling out-of-vocabulary words. \n",
    "- Instead, they use a Byte Pair Encoding (BPE) tokenizer, which breaks words down into smaller subword units, allowing them to process virtually any input text without the need for an explicit unknown-word token. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
