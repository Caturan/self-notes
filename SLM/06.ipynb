{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21f5f1f5",
   "metadata": {},
   "source": [
    "## Day 6: Building a Small Language Model from Scratch \n",
    "### What is Positional Embedding and Why It Matter? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912a3d75",
   "metadata": {},
   "source": [
    "## The Core Problem: Transformers Have No Sence of Order\n",
    "- At the heart of most modern language models lies the Transformer architecture, a structure that processes input as a set of tokens rather than a sequence. \n",
    "- Unlike RNNs (Recurrent Neural Networks), which read input word-by-word in order, Transformers look at all tokens at once, in parallel. That's great for speed, but here's the tradeoff: \n",
    "    - Transformers lack a built-in understanding of word order. \n",
    "- To a Transformer, \"I love AI\" is no different than \"AI love I\". \n",
    "- And that's huge problem, because meaning depends on order. So, how do we fix it? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affcd1eb",
   "metadata": {},
   "source": [
    "## But First.. What is an Embedding? \n",
    "- Before we jump into positional embeddings, let's take a moment to talk about embeddings in general, because they are everywhere in machine learning, especially in NLP. \n",
    "- An embedding is a method for representing discrete data (such as words, tokens or entire sentences) as dense, continous vectors in a high-dimensional space. \n",
    "- Why do we need this? \n",
    "- Because neural networks don't understand text. \n",
    "- They understand numbers. \n",
    "- So we take each word and turn it into a vector, one that captures its meaning context, and relationships to other words. \n",
    "    - For example, in a good embedding space: \n",
    "        - The vector for \"king\" minus \"man\" plus \"woman\" should land close to \"queen.\" \n",
    "        - Words like \"Paris\" and \"France\" will be near each other, as will \"Tokyo\" and \"Japan\". \n",
    "        - These word embeddings allow models to reason about relationships, analogies and meaning far beyond raw text. \n",
    "- Now that we understand what embeddings are, let's move on to the question of where each word appears, because the position matters too.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01efb413",
   "metadata": {},
   "source": [
    "##  The Fix: Positiona Embeddings \n",
    "- To give Transformers a sense of order, we inject some extra information into the input: positional embeddings. \n",
    "- Imagine we are feeding a sentence into the model. Each word is turned into a vector (thanks to word embeddings), but we also need to tell the model: \n",
    "    - \"This is the first word, this is the second, this is the third...\" \n",
    "- That's where positional embeddings come in; they are learnable (or sometimes fixed) vectors that are added to the word embeddings. \n",
    "- This combo of word meaning + word position is what the model uses to understand the sentence. \n",
    "- We can think of it like this: \n",
    "    - Final Input = Word Embedding + Positional Embedding \n",
    "- The simple addition gives the model a powerfull clue: not just what each word means, but where it appears in the sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71abbd",
   "metadata": {},
   "source": [
    "### Wait, Can't the Model Just Learn Order by Itself? \n",
    "- That's a great question. \n",
    "- In theory, a sufficiently large model could attempt to learn position solely by examining patterns. \n",
    "- However, in practice, that's inefficient and prone to error. \n",
    "- Positional embeddings serve as a helpful shortcut, providing the model with positional awareness from the outset. \n",
    "- Without them, models are just guessing order, like reading a book with all the pages shuffled. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fc3724",
   "metadata": {},
   "source": [
    "## How are Positional Embeddings Represented? \n",
    "- There are two main flavors of positional embeddings we will come across: \n",
    "1. Sinusoidal Positional Embeddings: (Used in original Transformer paper)\n",
    "    - These are fixed, not learned during training. They use sine and cosine functions at different frequencies to to create a unique position vector for each token. \n",
    "    - Why use sinusoids? Because they allow the model to generalize to longer sequences it hasn't seen before. \n",
    "    - They are elegant and mathematically clever. \n",
    "2. Learned Positional Embeddings: (Used in models like BERT)\n",
    "    - Here, the model learns the position vectors during training, just like it learn word meanings. \n",
    "    - This offers flexibility, but it means the model may struggle slightly with sequences longer than those it saw during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86925191",
   "metadata": {},
   "source": [
    "## What About New Techniques? \n",
    "- Recently, there's been a lot of innovation in this space. For example: \n",
    "- Rotary Positional Embeddings (RoPE) - used in models like DeepSeek and LLamMA. \n",
    "- These embed positional information directly into the attention mechanism and are particularly suitable for long-context scenarios. \n",
    "- RoPE approaches aim to address certain limitations of traditional positional embeddings, particularly in handling long sequences or cross-lingual understanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc1fbdd",
   "metadata": {},
   "source": [
    "# Final Thought \n",
    "- It's easy to overlook positional embeddings when talking about AI models, but they are absolutely essential. \n",
    "- Without them, Transformers would be like a GPS with no sense of direction --plenty of information, but no clue where anything goes. \n",
    "- So next time we are working with a model that feels like magic, remember: part of that magic comes from teaching the model not just what words mean, but where they belong. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
