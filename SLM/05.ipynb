{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffbce51c",
   "metadata": {},
   "source": [
    "# Day 5: Using tiktoken In LLM Workflows "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f17a093",
   "metadata": {},
   "source": [
    "- Congratualtions, we have reached Day 5. \n",
    "- So far, we have covered the fundementals of what a small language model is. \n",
    "- We built our first tokenizer from scratch and explored its limitations, particularly in handling unknown words. \n",
    "- This led us to the Byte Pair Encoding (BPE) tokenizer, which addresses many of those issues. \n",
    "- I also introduced the tiktoken module from OpenAI, and today, we will take a closer look at how we are using it in our code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82ea6e",
   "metadata": {},
   "source": [
    "## Using tiktoken: ThE GPT-2 BPE Tokenizer \n",
    "- To use the tiktoken library, we start by installing and importing it. \n",
    "- The GPT-2 tokenizer uses Byte Pair Encoding (BPE), which is: \n",
    "    - Fast \n",
    "    - Memory-efficient\n",
    "    - Pretrained on a wide range of English text \n",
    "- This makes it effective at breaking down words into meaningful subwords units. \n",
    "- For example, words like play, playing and played are represented in a compact and generalized way that improves downstream model performance. \n",
    "- GPT-2's tokenizer remains one of the best options for building and experimenting with small-to mid-sized language models, especially in early prototyping stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ebbd5e",
   "metadata": {},
   "source": [
    "## Special Tokens for Storytelling \n",
    "- Further down in the code, we use special tokens to help the model understand stroy structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67df57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "- self.special_tokens = {\n",
    "    \"start_story\": \"<|startofstory|>\",\n",
    "    \"end_story\": \"<|endofstory|>\",\n",
    "    \"title\": \"<|title|>\", \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5020b0",
   "metadata": {},
   "source": [
    "- These are vital for storytelling tasks. During both training and generation, these tokens tell the model:\n",
    "    - When to start and stop a story. \n",
    "    - Where the title goes\n",
    "    - When a new narrative section begins \n",
    "- These special tokens help the model think like a human storyteller by providing context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f14a5",
   "metadata": {},
   "source": [
    "##  Text Preprocessing \n",
    "- Before tokenizing, we need to clean and normalize the text.\n",
    "- The preprocess_text method, ensures consistency by converting text to lowercase, removing unnecessary whitespace, and replacing newlines with spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeeea5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(self, text):\n",
    "    # Basic text cleaning \n",
    "    text = text.lower() # Convert to lowercase for consisteny\n",
    "    text = text.replace('\\n', ' ') # Replace newlines with spaces \n",
    "    text = ' '.join(text.split()) # Normalize whitespace \n",
    "    return text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8ee263",
   "metadata": {},
   "source": [
    "- This function prepares both the prompt and the stroy sections of each example. \n",
    "- It helps reduce noise in the data and ensures that tokenization operates on clean, uniform input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f714e2b0",
   "metadata": {},
   "source": [
    "## Tokenization with Error Handling \n",
    "- The process method prepares each example by combining the prompt and story with special tokens. \n",
    "- It then tokenizes the text using the GPT-2 tokenizer from tiktoken. \n",
    "- To stay within the model's context limit, the tokenized output is truncated to a maximum of 1024 tokens. \n",
    "- If any error occurs during tokenization, it catches the exception and returns an empty sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c93c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(self, example):\n",
    "    # Preprocess both prompt and story\n",
    "    prompt = self.preprocess_text(example['prompt'])\n",
    "    story = self.preprocess_text(example['text'])\n",
    "    \n",
    "    # Create structured text with special tokens\n",
    "    full_text = (\n",
    "        f\"{self.special_tokens['prompt_start']} {prompt} {self.special_tokens['prompt_end']} \"\n",
    "        f\"{self.special_tokens['story_start']} {story} {self.special_tokens['story_end']}\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize with error handling\n",
    "    try:\n",
    "        ids = self.enc.encode_ordinary(full_text)\n",
    "        # Truncate to GPT-2's context limit\n",
    "        if len(ids) > 1024:\n",
    "            ids = ids[:1024]\n",
    "        return {'ids': ids, 'len': len(ids)}\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing text: {e}\")\n",
    "        return {'ids': [], 'len': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9443e",
   "metadata": {},
   "source": [
    "- This function ensures that every example is robustly tokenized, avoiding crashed during processing due to malformed input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ea534",
   "metadata": {},
   "source": [
    "## Dataset Loading and Splitting \n",
    "- The prepare_dataset method starts by downloading the Children Stories Collection dataset, using the Hugging Face datasets library. \n",
    "- It filters out examples that are too short or too long, and then splits the dataset into three subsets: train, validation and fine-tune. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f739ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ajibawa-2023/Children-Stories-Collection\")\n",
    "# Filter out too short/long examples\n",
    "def filter_by_length(example):\n",
    "    return 50 <= example['text_token_length'] <= 1000\n",
    "ds = ds.filter(filter_by_length)\n",
    "# Split the dataset: 80% train, 10% validation, 10% fine-tune\n",
    "train_val_test = ds[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "val_finetune = train_val_test[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "ds = {\n",
    "    \"train\": train_val_test[\"train\"],\n",
    "    \"validation\": val_finetune[\"train\"],\n",
    "    \"finetune\": val_finetune[\"test\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4606ab",
   "metadata": {},
   "source": [
    "- This ensures a well-balanced dataset within examples of appropriate length for training a small LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc90d9",
   "metadata": {},
   "source": [
    "## Parallel Tokenization and Binary Serialization \n",
    "- Once the dataset is split, each subset is tokenized using multiple processes (num_proc=8) to speed things up. \n",
    "- The resulting token IDs are then stored in .bin files using memory-mapped NumPy arrays. \n",
    "- This format allows for fast, efficient access during model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = split_data.map(\n",
    "    self.process,\n",
    "    remove_columns=['text', 'prompt', 'text_token_length'],\n",
    "    desc=f\"tokenizing {split_name} split\",\n",
    "    num_proc=8,\n",
    ")\n",
    "filename = os.path.join(self.data_dir, f\"{split_name}.bin\")\n",
    "arr_len = np.sum(tokenized['len'], dtype=np.uint64)\n",
    "dtype = np.uint16\n",
    "arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "total_batches = 1024\n",
    "idx = 0\n",
    "for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "    batch = tokenized.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "    arr_batch = np.concatenate(batch['ids'])\n",
    "    arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "    idx += len(arr_batch)\n",
    "arr.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50a232",
   "metadata": {},
   "source": [
    "- By using memory mapping and sharding, this step optimizes the preprocesing pipeline for scalability, even with large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
