{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa625e4",
   "metadata": {},
   "source": [
    "# Day 2 - Tokenizers: The Unsung Heroes of Language Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7defeda",
   "metadata": {},
   "source": [
    "- When we interact with ChatGPT or any other large language models (LLM), we type in human-readable text like: \n",
    "    - Hello, how are you? \n",
    "    - But here is a secret: these models have no idea what words are. \n",
    "- Before we input ever reaches the model, it goes through a crucial transformation by a componenet called a tokenizer. \n",
    "    - It breaks down your sentence into tokens, the atomic units the model can understand. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b30de",
   "metadata": {},
   "source": [
    "## What is a Token? \n",
    "- At a high level, a token can be: \n",
    "    - A word (e.g hello)\n",
    "    - A subword (e.g un, believ, able)  \n",
    "    - A character (in some models)\n",
    "    - Even a piece of punctuation or whitespace \n",
    "- Think of tokens like LEGO bricks. Individually, they are just plastic pieces, but put them together, and you can build houses, castles, or spaceships. \n",
    "- The tokenizer converts human language into a sequence of tokens (usually integers), which the model then uses for training or inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ebacad",
   "metadata": {},
   "source": [
    "## Tokenization Techniques: The Building Blocks \n",
    "- There are three main ways models tokenize text. Let's break them down: \n",
    "1. Whitespace or Word-Level Tokenization (Basic and Rare Now):\n",
    "    - Splits by spaces and punctuation. Simple, but inefficient for handling unknown or rare words. \n",
    "    - Example:\n",
    "        - \"unbelievable\" -> [\"unbelievable\"]\n",
    "    - Note: If the model hasn't seen unbelievable before, it's stuck. \n",
    "\n",
    "2. Character-Level Tokenization (High Flexibility, Low Efficiency)\n",
    "- Breaks everything into individual characters. It works for any language, but the sequences become very long. \n",
    "    - Example: \n",
    "        - \"unbelievable\" → [\"u\", \"n\", \"b\", \"e\", \"l\", \"i\", \"e\", \"v\", \"a\", \"b\", \"l\", \"e\"]\n",
    "- Great for robust handling, but slow and hard to model long-term structure. \n",
    "\n",
    "3. Subword Tokenization (The Gold Standard)\n",
    "- Used by almost all modern LLMs. It breaks word into frequent chunk based on the training corpus. This allows it to handle:\n",
    "    - Common words as single tokens. \n",
    "    - Rare or made-up words using combinations. \n",
    "- Popular algorithms include: \n",
    "    - Byte-Pair Encoding (BPE) - Used in GPT-2, GPT-3, RoBERTa\n",
    "    - WordPiece - Used in BERT\n",
    "    - Unigram Language Model - Used in SentencePiece / T5\n",
    "    - GPT-4 uses a variant called tiktoken, which is optimized for performance. \n",
    "- Example with BPE:\n",
    "    - \"unbelievable\" → [\"un\", \"believ\", \"able\"]\n",
    "    - \"unicornify\" → [\"un\", \"icorn\", \"ify\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a2e52d",
   "metadata": {},
   "source": [
    "## Under the Hood: How a Tokenizer Actually Works\n",
    "- Let's unpack what happens when we run: \n",
    "    - tokenizer.encode(\"Hello, world!\")\n",
    "- Here's what happens step-by-step: \n",
    "1. Normalization: \n",
    "-   The text is cleaned and standardized, with lowercase letters, extra spaces removed, and Unicode normalization applied. \n",
    "    - \"Hello, world!\" -> \"hello, world!\" \n",
    "2. Pre-tokenization: \n",
    "- Text is split based on predefined rules (e.g., whitespace, puctuation). This is language-dependent. \n",
    "    - \"hello, world!\" → [\"hello\", \",\", \"world\", \"!\"]\n",
    "3. Subword Tokenization: \n",
    "- Now the magic begins. Each piece is matched to subword units from a learned vocabulary.\n",
    "    - \"hello\" → [\"he\", \"llo\"]\n",
    "    - \"world\" → [\"wor\", \"ld\"]\n",
    "- Each subword is mapped to a token (an integer). \n",
    "### Real Output\n",
    "- Lets say tokenizer has these mappings: \n",
    "- Token ID   he   42\n",
    "-          llo  91\n",
    "-          wor  57\n",
    "-          ld   82\n",
    "-          ,    11\n",
    "-          !    99\n",
    "\n",
    "- The final output becomes:\n",
    "    - [42, 91, 11, 57, 82, 99]\n",
    "- That's what our models actually sees, not the words, but these numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf024363",
   "metadata": {},
   "source": [
    "##  Vocabulary: Why It Matters\n",
    "- Each tokenizer has a fixed-size vocabulary, often around 32k to 50k tokens. \n",
    "- If your vocabulary is too small -> more tokens are needed per sentence. \n",
    "- If your vocabulary is too big -> harder to train, and requires more memory. \n",
    "- That's why models often optimize for the sweet spot, using tools like SentencePiece or tiktoken to find balance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3840c60",
   "metadata": {},
   "source": [
    "## Why Tokenization Affects Model Performance \n",
    "- Context Length: If tokenization is inefficient (splits too much), we run out of context window faster. \n",
    "- Training Cost: More tokens = More Compute \n",
    "- Hallucinations: Poor tokenization can cause weird generation if the model can't map input cleanly. \n",
    "- For example, ChatGPT might be tokenized as [\"Chat\", \"G\", \"PT\"] by a one tokenizer and [\"ChatGPT\"] by another -leading to different responses.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd302ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57bb4c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\info\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1742939a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unbelievable', 'scenes', '!']\n",
      "[23653, 5019, 999]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"Unbelievable scenes!\")\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(tokens)\n",
    "print(ids) # Added to show the IDs as well, which is often useful"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
