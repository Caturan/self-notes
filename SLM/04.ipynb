{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b0439a",
   "metadata": {},
   "source": [
    "# Day 4 - Understanding Byte Pair Encoding (BPE) Tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94851d",
   "metadata": {},
   "source": [
    "- So far, we explored what a tokenizer is and even built our own from scratch. \n",
    "- However, one of the key limitations of building a custom tokenizer is handling unknown or rare words. \n",
    "- This is where adcanced tokenizers like OpenAI's tiktoken, which uses Byte Pair Encoding (BPE), really shine. \n",
    "- We also understood, Language models don't read or understand in the same way humans do. \n",
    "- Before any text can be processed by a model, it needs to be tokenized, that is, broken into smaller chunks called tokens. \n",
    "- One of the most efficient and widely adopted techniques to perform this is called Byte Pair Encoding (BPE). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987e5c7d",
   "metadata": {},
   "source": [
    "## What is Byte Pair Encoding? \n",
    "- Byte Pair Encoding is a data compression algorithm adapted for tokenization.\n",
    "- Instead of treating words as whole units, it breaks them down into smaller, more frequent subword units. This allows it to:\n",
    "    - Handle unknown words gracefully \n",
    "    - Strike a balance between character-level and word-level tokenization. \n",
    "    - Reduce the overall vocabulary size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df9a2e0",
   "metadata": {},
   "source": [
    "## How BPE Works\n",
    "### Step 1: Start with Characters \n",
    "- We begin by breaking all words in our corpus into character: \n",
    "- \"low\", \"lower\", \"newest\", \"widest\"\n",
    "- → [\"l\", \"o\", \"w\"], [\"l\", \"o\", \"w\", \"e\", \"r\"], ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c07f23f",
   "metadata": {},
   "source": [
    "### Step 2: Count Pair Frequencies\n",
    "- We count the frequency of adjacent character pairs (biagrams). \n",
    "- \"l o\": 2, \"o w\": 2, \"w e\": 2, \"e s\": 2, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1527bb",
   "metadata": {},
   "source": [
    "### Step 3: Merge the Most Frequent Pair \n",
    "- Merge the most frequent pair into a new token: \n",
    "- Merge \"e s\" → \"es\"\n",
    "- Now \"newest\" becomes: [\"n\", \"e\", \"w\", \"es\", \"t\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3033ec2f",
   "metadata": {},
   "source": [
    "### Step 4: Repeat Until Vocabulary Limit\n",
    "- Continue this process until we reach the desired vocabulary size or until no more merges are possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f024663",
   "metadata": {},
   "source": [
    "## Why is BPE Powerful? \n",
    "- Efficient: It reuses frequent subwords to reduce redundancy. \n",
    "- Flexible: Handlers rare and compound words better that word-level tokenizers. \n",
    "- Compact Vocabulary: Essential for performance in large models. \n",
    "- It solves a key problem: How to tokenize unknown or rare words without bloating the vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e8d65",
   "metadata": {},
   "source": [
    "## Example: Using tiktoken for BPE Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b59e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "638dd623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [40, 56188, 1687, 7403, 374, 4857, 264, 47058, 1701, 426, 1777]\n",
      "Decoded Text: IdeaWeaver is building a tokenizer using BPE\n",
      "Tokens ['I', 'dea', 'We', 'aver', ' is', ' building', ' a', ' tokenizer', ' using', ' B', 'PE']\n"
     ]
    }
   ],
   "source": [
    "import tiktoken \n",
    "\n",
    "encoding = tiktoken.get_encoding (\"cl100k_base\")\n",
    "\n",
    "text = \"IdeaWeaver is building a tokenizer using BPE\"\n",
    "\n",
    "token_ids = encoding.encode(text)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "decoded_text = encoding.decode(token_ids)\n",
    "print(\"Decoded Text:\", decoded_text)\n",
    "\n",
    "tokens = [encoding.decode([id]) for id in token_ids]\n",
    "print(\"Tokens\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b0334",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "- Byte Pair Encoding may sound simple, but it's one of the key innovations that made today's large language models possible. \n",
    "- It strikes a balance between efficieny, flexibility, and robustness in handling diverse language input. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
