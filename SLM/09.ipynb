{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8dc1a0",
   "metadata": {},
   "source": [
    "# Day 9: Coding Rotary Positional Embeddings (RoPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc5a9c",
   "metadata": {},
   "source": [
    "## Quick Recap: What is RoPE? \n",
    "- RoPE is a method for injection positional information into tranformer models, not by adding position vectors (like absolute positional embeddings), but by rotating the query and key vectors within the attention mechanism. \n",
    "- This provides several advantages: \n",
    "    - Relative Position Awareness: Understands the distance between tokens.\n",
    "    - Extrapolation: Handles sequences longer than seen during training. \n",
    "    - Efficiency: Doesn't require additional embeddings -just math inside attention "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c271404",
   "metadata": {},
   "source": [
    "## Code Walkthrough \n",
    "-  Letâ€™s walk through how RoPE is implemented in the DeepSeek-Children-Stories-15M-model https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model codebase. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7337c53f",
   "metadata": {},
   "source": [
    "### 1. Implementation: RoPEPositionalEncoding \n",
    "- In the file src/model/deepseek.py, we will find the class RoPEPositionalEncoding. \n",
    "- This class:  \n",
    "    - Precomputes rotation frequencies \n",
    "    - Provides an apply_rope method. \n",
    "    -Applies RoPE to input tensors, usually the query and key vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepseek.py \n",
    "class RoPEPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=2048):\n",
    "        super().__init__()\n",
    "        inv_freq= 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_len, dtype=torch.float)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
    "        emb = torch.cat((freqs.sin(), freqs.cos()), dim=-1)\n",
    "        self.register_buffer(\"positional encoding\", emb)\n",
    "\n",
    "    def apply_rope(self, x, position_ids):\n",
    "        rope = self.positional_encoding[position_ids]\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        rope1, rope2 = rope[..., ::2], rope[..., 1::2]\n",
    "        return torch.cat([x1 * rope2 + x2 * rope1, x2 * rope2 - x1 * rope1], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca73bac8",
   "metadata": {},
   "source": [
    "- The key idea is rotating even and odd dimensions of the query/key vectors based on sine and cosine frequencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4d0ca",
   "metadata": {},
   "source": [
    "### 2: Usage: Integrating RoPE into Attention \n",
    "- The DeepSeek model utilizes a custom attention mechanism known as Multihead Latent Attention (MLA). \n",
    "- Here's how RoPE is integrated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16874f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepseek.py \n",
    "q = self.q_proj(x)\n",
    "k = self.k_proj(x)\n",
    "\n",
    "q = self.rope.apply_rope(q, positions_ids)\n",
    "k = self.rope.apply_rope(k, position_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd2e21",
   "metadata": {},
   "source": [
    "- What's happening? \n",
    "    - x is projected into query (q) and key (k) vectors. \n",
    "    - RoPE is applied to both using apply_rope, injecting position awarenness. \n",
    "    - Attention proceeds as usual --except now the queries and keys are aware of their relative positions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa94d2",
   "metadata": {},
   "source": [
    "### 3: Where RoPe is used \n",
    "- Every Transformer Block: Each block in the DeepSeek model uses MLA and applies RoPE. \n",
    "- During Both Training and Inference: RoPE is always on, helping the model understand the token sequences no matter the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202aaa27",
   "metadata": {},
   "source": [
    "## Why RoPE is Perfect for Story Generation \n",
    "- In story generation, especially for children's stories, context is everything. \n",
    "- RoPE enables the model to:    \n",
    "    - Track who did what across paragraphs \n",
    "    - Maintain chronological consistency\n",
    "    - Preserve narrative flow even in long outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35520ade",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "- Rotary Positional Embeddings (RoPE) are not just a theoratical improvement; they offer practical performance and generalization benefits. \n",
    "-  If we are working on any transformer-based task with long sequences, story generation, document QA, or chat history modeling, we should absolutely consider using RoPE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
