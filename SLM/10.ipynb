{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73933349",
   "metadata": {},
   "source": [
    "# Day 10: What is Model Distillation? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b98900",
   "metadata": {},
   "source": [
    "- This is one of my favorite topics. I have always wanted to run large models (like DeepSeek 671b) or at least make my smaller models behave as intelligently and powerfully as those massive, high parameter models. \n",
    "- But like many of us, we dont always have the hardware to run those resource-intensive models. \n",
    "- But what if we could transfer the knowledge of a large model to a smaller one? \n",
<<<<<<< HEAD
    "- That is the whole idea of model distillation."
=======
    "- That is the whole idea of model distillation. "
>>>>>>> 9c79fb08b946c53975b7c33b49022f3403b34552
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f5d11d",
   "metadata": {},
   "source": [
    "## What is Model Distillation? \n",
    "- Model distillation is a technique in which large, complex model (referred as a teacher) transfers its knowledge to a smaller, simpler model (referred as the student). \n",
    "- The goal is to make the student model perform almost as well as the teacher, but with fewer resources. \n",
    "- Think of it like this: A PhD professor (teacher model) teaches a high school student (student model) everything they know, without the student having to go through a decade of research. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addcb0bc",
   "metadata": {},
   "source": [
    "## Why Do We Need Model Distillation? \n",
    "- Large models are:     \n",
    "    - Expensive to run \n",
    "    - Hard to deploy on edge devices \n",
    "\n",
    "- Distillation solve this by:\n",
    "    - Lowering memory/compute usage \n",
    "    - Maintaining competitive accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873728d3",
   "metadata": {},
   "source": [
    "## How Does Model Distillation Work? \n",
    "- There are three main components: \n",
    "1. Teacher Model: A large, pre-trained model with high performance \n",
    "2. Student Model: A smaller model, which we aim to train to mimic the teacher. \n",
    "3. Soft Targets: Instead of just learning from the ground-truth labels, the student also learns from the teacher's probability distribution over classes (logits), which carries extra information. \n",
    "- Let me break it down in simple language. In the case of traditional training, the model learns from hard labels. \n",
    "- For example, if the correct answer is \"Cat\", the label is  simply 1 for \"Cat\" and 0 for everything else. \n",
    "- However, in model distillation, the student also learns from the teacher's soft predictions, which means  it not only knows the correct answer but also how confident the teacher is about each possible answer.\n",
    "- Let me provide a simple example: \n",
    "- Let's say the task is image classification. \n",
    "- Image: Picture of a cat \n",
    "- Hard Label (ground truth):\n",
    "    - \"Cat\" -> 1 \n",
    "    - All other classes -> 0 \n",
    "- Teacher model's prediction (soft label): \n",
    "    - \"Cat\" -> 85%\n",
    "    - \"Dog\" -> 10%\n",
    "    - \"Fox\" -> 4%\n",
    "    - \"Rabbit\" -> 1%\n",
    "- Instead of learning only \"This is a Cat\", the student model also learns that: \n",
    "    - \"The teacher is very confident it's a cat, but it's also somewhat similar to a dog or a fox\". \n",
    "- This addtional information helps, students learn more nuances decision boundaries, making them more innovative and generalizable, even with fewer parameters. \n",
    "- To sum up, Distillation allows the student to model learning not just what the teacher thinks is correct, but also how confident the teacher is across all options; this is what we call learning from soft targets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3821b96",
   "metadata": {},
   "source": [
    "## Types of Knowledge Distillation \n",
    "- There is more than one way to pass knowledge from a teacher to a student. Let's look at the main types: \n",
    "1. Logit-based Distillation (Hinton et al.):\n",
    "    - This is the method introduces by Geoffrey Hinton, the father of deep learning. \n",
    "    - Here, the student doesn't just learn from the correct label, but from the full output of the teacher (called logits), which contains rich information about how confident the teacher is in each class. \n",
    "        - Thinks of it like learning how the teacher thinks, not just what the final answer is. \n",
    "2. Feature-based Distillation: \n",
    "    - Instead of copying the final output, the student attempts to mimic the intermediate representations (such as hidden layers) of the teacher model. \n",
    "        - Imagine learning how the teacher breaks down and anaylzes the problem step by step, rather than just their final conclusion. \n",
    "    - This is useful when we want to student to develop a similar internal understanding to that of the teacher. \n",
    "3. Response-based Distillation: \n",
    "    - This one one is more straightforward; the student is trained to match the teacher's final output, often without worrying about logits or hidden features. \n",
    "        - It's like learning to copy the teacher's answer sheet during a test -- not the most comprehensive learning, but sometimes good enough for quick tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57997a7",
   "metadata": {},
   "source": [
    "## Real-World Applications -- Why Distillation Matters\n",
    "- Mobile Devices\n",
    "    - Want to run BERT or GPT on your phone without needing a cloud GPU? \n",
    "    - Distillation models make this possible by reducing the size of large models while preserving much of their power. \n",
    "- Autonomous Vehicles:  \n",
    "    - Edge devices in self-driving cars can't afford slow, bulky models. \n",
    "    - Distilled vision models enable faster, real-time decisions without reaquiring a massive compute stack in the trunk. \n",
    "- Chatbots and Virtual Assistans: \n",
    "    - For real-time conservations, low latency is key. Distilled language models offer fast responses while maintaining low memory and compute usage, making them ideal for customer service bots or AI tutar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b853c939",
   "metadata": {},
   "source": [
    "## Limitations and Challenges\n",
    "1. Performance Gap: \n",
    "    - Despite the best efforts, a student model may not accurately match the teacher's performance, especially on complex tasks that require fine-grained reasoning. \n",
    "2. Architecture Mismatch: \n",
    "    - If the student model is too different from the teacher in design, it may struggle to \"undertand\" what the teacher is trying to teach. \n",
    "3. Training Overhead:   \n",
    "    - Training a good student model still takes time, data, and effort; it's not a simple copy-paste job. And sometimes, tuning distillation hyperparameters (such as temperature or alpha) can be tricky. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc6ad0a",
   "metadata": {},
   "source": [
    "## Popular Tools and Frameworks \n",
    "- Hugging Face: \n",
    "    - Models like DistilBERT are smaller and faster versions of BERT, trained via distillation. \n",
    "- TinyML: \n",
    "    - This focuses on deploying distilled models on ultra-low-power-devices, such as microcontrollers, think smartwatches or IoT sensors. \n",
    "- OpenVINO / TensorRT: \n",
    "    - These are optimization toolkits by Intel and NVDIA that pair well with distilled models to extract every last bit of performance from them on CPUs and GPUs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
